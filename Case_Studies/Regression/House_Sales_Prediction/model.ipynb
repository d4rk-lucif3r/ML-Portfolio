{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('kc_house_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Correlation among the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_data = dataset.drop(['id','date','price'], axis = 1)\n",
    "# plt.figure(figsize=(20, 17))\n",
    "# matrix = np.triu(corr_data.corr())\n",
    "# sns.heatmap(corr_data.corr(), annot=True,\n",
    "#             linewidth=.8, mask=matrix, cmap=\"rocket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().T.style.bar(\n",
    "    subset=['mean'],\n",
    "    color='#606ff2').background_gradient(\n",
    "    subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = dataset.drop(['id', 'date', ], axis=1)\r\n",
    "# fig = plt.figure(figsize=(20, 20))\r\n",
    "# for i in range(len(plot_data.columns)):\r\n",
    "#     fig.add_subplot(np.ceil(len(plot_data.columns)/5), 5, i+1)\r\n",
    "#     plot_data.iloc[:, i].hist(bins=20)\r\n",
    "#     plt.title(plot_data.columns[i])\r\n",
    "#     fig.tight_layout(pad=3.0)\r\n",
    "# plt.show()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributionPlot(dataset):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(dataset.columns)):\n",
    "        fig.add_subplot(np.ceil(len(dataset.columns)/5), 5, i+1)\n",
    "        sns.distplot(\n",
    "            dataset.iloc[:, i], color=\"lightcoral\", rug=True)\n",
    "        fig.tight_layout(pad=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributionPlot(plot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['floors'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pieChartPlotter(dataset, columnName):\n",
    "    values = dataset[columnName].value_counts()\n",
    "    labels = dataset[columnName].unique()\n",
    "    pie, ax = plt.subplots(figsize=[10, 6])\n",
    "\n",
    "    patches, texts, autotexts = ax.pie(values, labels=labels, autopct='%1.2f%%', shadow=True, pctdistance=.5, explode=[0.06]*dataset[columnName].unique()\n",
    "                                       )\n",
    "\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.title(columnName, color='white', fontsize=14)\n",
    "    plt.setp(texts, color='white', fontsize=20)\n",
    "    plt.setp(autotexts, size=10, color='white')\n",
    "    autotexts[1].set_color('white')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieChartPlotter(dataset,'waterfront')\n",
    "# pieChartPlotter(dataset, 'floors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPlotter(dataset):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(dataset.columns)):\n",
    "        if not dataset.columns[i] == 'price':\n",
    "            fig.add_subplot(np.ceil(len(dataset.columns)/2), 2, i)\n",
    "            sns.countplot(dataset[dataset.columns[i]],\n",
    "                          order=dataset[dataset.columns[i]].value_counts().index)\n",
    "\n",
    "            fig.tight_layout(pad=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_plot_data = dataset.drop(\n",
    "    ['id', 'date', 'sqft_living15', 'sqft_lot15', 'lat', 'long', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countPlotter(group_plot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupBarPlotter(dataset):\n",
    "    # groups = dataset.groupby([column])['price'].mean()\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(dataset.columns)):\n",
    "        if not dataset.columns[i] == 'price':\n",
    "            groups = dataset.groupby(dataset.columns[i])['price'].mean()\n",
    "            fig.add_subplot(np.ceil(len(dataset.columns)/2), 2, i)\n",
    "            plt.xlabel('price')\n",
    "            groups.plot.barh()\n",
    "            fig.tight_layout(pad=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBarPlotter(group_plot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = dataset.drop(['id', 'date','price'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewnessCorrector(dataset,columnName):\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    from scipy.stats import norm, boxcox\n",
    "    \"\"\"\n",
    "    This function returns two plots distplot and probability plot for non-normalized data and after normalizing the provided data. \n",
    "    Just provide it with two parameters dataset and the name of column.\n",
    "    It corrects the skewness of data applying Boxcox transformation on the provided data\n",
    "    Example:\n",
    "    1) Single Column\n",
    "        skewnessCorrector(Dataset,'XYZ')\n",
    "    2) Multiple Columns\n",
    "        skewColumnList = ['ABC',\n",
    "                  'DEF', 'GHI']\n",
    "        for column in skewColumnList:\n",
    "            skewnessCorrector(column)\n",
    "    \"\"\"\n",
    "    print('''Before Correcting''')\n",
    "    (mu, sigma) = norm.fit(dataset[columnName])\n",
    "    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n",
    "        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n",
    "    plt.title(columnName.capitalize() +\n",
    "              \" Distplot before Skewness Correction\", color=\"black\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(dataset[columnName], plot=plt)\n",
    "    plt.show()\n",
    "    # Applying BoxCox Transformation\n",
    "    dataset[columnName], lam_fixed_acidity = boxcox(\n",
    "        dataset[columnName])\n",
    "    \n",
    "    print('''After Correcting''')\n",
    "    (mu, sigma) = norm.fit(dataset[columnName])\n",
    "    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n",
    "        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n",
    "    plt.title(columnName.capitalize() +\n",
    "              \" Distplot After Skewness Correction\", color=\"black\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(dataset[columnName], plot=plt)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewcolumns = [ 'sqft_living', 'sqft_lot', 'floors',\n",
    "                'condition', 'grade', 'sqft_above',\n",
    "                 'lat',\n",
    "               'sqft_living15', 'sqft_lot15']\n",
    "for column in skewcolumns:\n",
    "    skewnessCorrector(house,column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house.values\n",
    "y = dataset.iloc[:, 2:3].values\n",
    "colnames = house.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection via RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store our rankings\n",
    "ranks = {}\n",
    "# Create our function which stores the feature rankings to the ranks dictionary\n",
    "\n",
    "\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    if np.array(ranks).ndim == 1:\n",
    "        ranks = np.array(ranks).reshape(1, -1)\n",
    "    ranks = minmax.fit_transform(order*np.array(ranks).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = {}\n",
    "def featureRanker(X, y, ranking, colnames):\n",
    "    \"\"\"\"\"\"\n",
    "    params = {}\n",
    "    models = {\n",
    "        LinearRegression(**params): [{'normalize': True}, 'lr'],\n",
    "        Ridge(**params): [{'alpha': 7}, 'Ridge'],\n",
    "        Lasso(**params): [{'alpha': .05}, 'Lasso'],\n",
    "        ElasticNet(**params): [{'alpha': 0.0005, 'l1_ratio': .9, 'random_state': 0}, 'Elastic'],\n",
    "        SVR(**params): [{'kernel': 'rbf'}, 'SVR'],\n",
    "        RandomForestRegressor(**params): [{'n_jobs': -1,\n",
    "                                           'n_estimators': 100, 'random_state': 0}, 'RF'],\n",
    "        GradientBoostingRegressor(**params): [{'n_jobs': -1,\n",
    "                                               'n_estimators': 100, 'random_state': 0}, 'GBR'],\n",
    "        XGBRegressor(**params): [{'n_jobs': -1,\n",
    "                                  'n_estimators': 100, 'random_state': 0}, 'XGBR'],\n",
    "        LGBMRegressor(**params): [{'n_jobs': -1,\n",
    "                                   'n_estimators': 100, 'random_state': 0}, 'LGBM'],\n",
    "    }\n",
    "    for i, model in enumerate(models):\n",
    "        params = models[model][0]\n",
    "        estimator = model\n",
    "        print(model, params)\n",
    "        estimator.fit(X, y)\n",
    "        if models[model][1] == 'lr':\n",
    "            rfe = RFE(estimator, n_features_to_select=1)\n",
    "            rfe.fit(X, y)\n",
    "            ranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n",
    "        if not hasattr(estimator, 'coef_'):\n",
    "            if not hasattr(estimator, 'dual_coef_'):\n",
    "                ranks[models[model][1]] = ranking(\n",
    "                    estimator.feature_importances_, colnames)\n",
    "            elif hasattr(estimator, 'dual_coef_'):\n",
    "                ranks[models[model][1]] = ranking(\n",
    "                    np.abs(svr.dual_coef_), colnames)\n",
    "        elif hasattr(estimator, 'coef_'):\n",
    "            ranks[models[model][1]] = ranking(\n",
    "                np.abs(estimator.coef_), colnames)\n",
    "    return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = featureRanker(X, y, ranking, colnames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Feature Ranking Matrix\n",
    "\n",
    "We combine the scores from the various methods above and output it in a matrix form for convenient viewing as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to store the mean value calculated from all the scores\n",
    "r = {}\n",
    "for name in colnames:\n",
    "    r[name] = round(np.mean([ranks[method][name]\n",
    "                             for method in ranks.keys()]), 2)\n",
    "\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    "\n",
    "print(\"\\t%s\" % \"\\t\".join(methods))\n",
    "for name in colnames:\n",
    "    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str,\n",
    "                                          [ranks[method][name] for method in methods]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the matrix above, the numbers and layout does not seem very easy or pleasant to the eye. Therefore, let's just collate the mean ranking score attributed to each of the feature and plot that via Seaborn's factorplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the mean scores into a Pandas dataframe\n",
    "meanplot = pd.DataFrame(list(r.items()), columns=['Feature', 'Mean Ranking'])\n",
    "\n",
    "# Sort the dataframe\n",
    "meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the ranking of the features\n",
    "sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data=meanplot,\n",
    "               kind=\"bar\", size=4, aspect=1.9, palette='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well as you can see from our feature ranking endeavours, the top 3 features are 'lat', 'grade' and 'waterfront'. The bottom 3 are 'yr_renovated', 'sqft_lot15' and 'sqft_lot' . This sort of feature ranking can be really useful, especially if one has many many features in the dataset and would like to trim or cut off features that contribute negligibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = house.loc[:,['lat', 'grade', 'waterfront']].values\n",
    "target = dataset.loc[:,['price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test = {}\n",
    "rmse_train={}\n",
    "score={}\n",
    "def predictor(X_train, X_test ,y_train, y_test):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        LinearRegression(normalize= True):  'lr',\n",
    "        Ridge(alpha= 7):  'Ridge',\n",
    "        Lasso(alpha= .05):  'Lasso',\n",
    "        ElasticNet(alpha= 0.0005, l1_ratio= .9, random_state= 0):  'Elastic',\n",
    "        RandomForestRegressor(n_jobs= -1,\n",
    "                               n_estimators= 100, random_state= 0):  'RF',\n",
    "        GradientBoostingRegressor(n_estimators= 100, random_state= 0):  'GBR',\n",
    "        XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                      colsample_bytree=1, max_depth=70, random_state = 0):  'XGBR',\n",
    "        LGBMRegressor(n_jobs= -1,\n",
    "                       n_estimators= 100, random_state= 0):  'LGBM',\n",
    "    }\n",
    "    for i, model in enumerate(models):\n",
    "\n",
    "      \n",
    "        estimator = model\n",
    "        estimator.fit(X_train, y_train)\n",
    "        ytrain_pred = estimator.predict(X_train)\n",
    "        y_pred = estimator.predict(X_test)\n",
    "\n",
    "        rmse_train[models[model]] = mean_squared_error(\n",
    "            y_train, ytrain_pred, squared=False)\n",
    "        rmse_test[models[model]] = mean_squared_error(\n",
    "            y_test, y_pred, squared=False)\n",
    "        score[models[model]] = estimator.score(X_test, y_test)\n",
    "    return rmse_train, rmse_test, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train, rmse_test, score = predictor(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBRegressor(\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth': range(2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.01, 0.05]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=10,\n",
    "    cv=10,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2951d919548ae18de2df5ded62b9a6d18db65d3b5a4ffaab87b83029c7c31e33"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}